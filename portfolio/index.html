<!DOCTYPE html>
<html lang="fr">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Portfolio | LONDE  Tristan</title>
    <meta name="author" content="LONDE  Tristan">
    <meta name="description" content="Voici mon portfolio
">
    <meta name="keywords" content="portfolio, projets, tristanlonde, français, ia, ai">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.jpg">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tirovo.github.io//portfolio/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header --><header>
    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
            <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">LONDE </span>Tristan</a>
            <!-- Navbar Toggle -->
            <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar top-bar"></span>
                <span class="icon-bar middle-bar"></span>
                <span class="icon-bar bottom-bar"></span>
            </button>

            <div class="collapse navbar-collapse text-right" id="navbarNav">
                <ul class="navbar-nav ml-auto flex-nowrap">

                    <!-- About -->
                    <li class="nav-item ">
                        <a class="nav-link" href="/">About</a>
                    </li>
                    

                    <!-- Other pages -->
                    <li class="nav-item ">
                        <a class="nav-link" href="/projects/">Projects</a>
                    </li>
                    <li class="nav-item ">
                        <a class="nav-link" href="/resume/">Resume</a>
                    </li>

                    <!-- Toggle theme mode -->
                    <li class="toggle-container">
                        <button id="light-toggle" title="Change theme">
                            <i class="fas fa-moon"></i>
                            <i class="fas fa-sun"></i>
                        </button>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</header>

<script>
    // Vérifie si l'utilisateur est sur Google Traduction
    const urlPattern = /^https:\/\/tirovo-github-io\.translate\.goog.*\?_x_tr_sl=en&_x_tr_tl=fr&_x_tr_hl=fr$/;
    if (urlPattern.test(window.location.href)) {
        // Remplace le bouton par le bouton anglais
        const languageToggle = document.getElementById('language-toggle');
        languageToggle.innerHTML = `
            <a class="nav-link" href="https://tirovo.github.io/" style="padding-bottom: 0.4rem;padding-top: 0.4rem;padding-right: 0px;">
                <img alt="EN" title="EN" src="https://tirovo.github.io/assets/img/languages/en.gif" style="padding-left: 6px;padding-right: 0px;">
            </a>
        `;
    }
</script>


    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            <span class="font-weight-bold">LONDE</span> Tristan
          </h1>
        </header>

        <article>
          <div class="clearfix">
            
<p>👋 Hello!</p>

<p>I’m an Artificial Intelligence engineering student at <strong>ENIB</strong> (École Nationale d’Ingénieurs de Brest), passionate about <strong>speech synthesis</strong>, <strong>signal processing</strong>, <strong>computer vision</strong>, and building <strong>offline intelligent systems</strong>.</p>

<p>In my free time, I design and develop <strong>fully local voice assistants</strong>, <strong>machine learning applications for voice</strong>, and innovative projects at the intersection of <strong>cybersecurity</strong>, <strong>computer vision</strong>, and <strong>AI-generated content detection</strong> (like <strong>deepfake audio</strong>).</p>

<p>I specialize in creating <strong>privacy-focused local voice solutions</strong> and explore open-source LLMs like <strong>GPT4All</strong>, <strong>Nous Hermes</strong>, and <strong>Mistral</strong>. I also enjoy working with tools like <strong>FastAPI</strong>, <strong>Streamlit</strong>, and <strong>OpenCV</strong> to build <strong>real-time AI-powered interfaces</strong>.</p>

<p>🌍 Interests</p>

<ul>
  <li>💻 <strong>AI &amp; Machine Learning</strong>: Real-time models, voice AI, local-first systems</li>
  <li>🎤 <strong>Speech Synthesis</strong>: Custom voice assistants, voice-to-voice pipelines</li>
  <li>👁️ <strong>Computer Vision</strong>: Fatigue detection, facial analysis, eye tracking</li>
  <li>🛡️ <strong>Cybersecurity</strong>: Deepfake detection, audio anti-spoofing</li>
</ul>

<hr>

<!-- pages/projects.md -->
<div class="projects">
  <!-- Parcours des catégories -->
      
      <!-- Récupération de l'ordre des sous-catégories définies --><div class="post page-break">
  <header class="post-header">
    <h1 class="post-title">EmotionAI-voice
      
        <a href="https://github.com/Tirovo/EmotionAI-voice" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fab fa-github" style="margin-left: 10px;margin-right: 10px;"></i></a>
      
    </h1>
    <p class="post-description">An AI-powered application for detecting human emotions</p>
  </header>

  <article>
    <p><img src="/assets/img/projects/EmotionAI-voice/main.png" width="300" height="auto"></p>

<p><strong>EmotionAI Voice</strong> is an open-source deep learning project that classifies vocal emotions using raw <code class="language-plaintext highlighter-rouge">.wav</code> audio.<br>
It’s designed for applications in mental health monitoring, UX analysis, and intelligent speech interfaces.</p>

<p>🔬 The model is trained <strong>from scratch</strong>, using spectrogram-based audio features, and aims to recognize 8 core emotions.</p>

<hr>

<h2 id="-features">🎯 Features</h2>

<ul>
  <li>🧠 Emotion recognition: <code class="language-plaintext highlighter-rouge">neutral</code>, <code class="language-plaintext highlighter-rouge">calm</code>, <code class="language-plaintext highlighter-rouge">happy</code>, <code class="language-plaintext highlighter-rouge">sad</code>, <code class="language-plaintext highlighter-rouge">angry</code>, <code class="language-plaintext highlighter-rouge">fearful</code>, <code class="language-plaintext highlighter-rouge">disgust</code>, <code class="language-plaintext highlighter-rouge">surprised</code>
</li>
  <li>🎧 Accepts <code class="language-plaintext highlighter-rouge">.wav</code> audio inputs (from RAVDESS dataset)</li>
  <li>📊 CNN and CNN+GRU models implemented in PyTorch</li>
  <li>🔍 Real-time evaluation with confusion matrix and accuracy tracking</li>
  <li>🛠️ Fully open-source and customizable (no pre-trained models)</li>
  <li>🧪 Includes <strong>SpecAugment</strong> for data augmentation (frequency/time masking)</li>
</ul>

<hr>

<h2 id="-dataset--ravdess">📚 Dataset — RAVDESS</h2>

<p>We use the <a href="https://zenodo.org/record/1188976" rel="external nofollow noopener" target="_blank"><strong>RAVDESS</strong> dataset</a>, which includes:</p>

<ul>
  <li>🎭 24 professional actors (balanced male/female)</li>
  <li>🎙️ 1440 <code class="language-plaintext highlighter-rouge">.wav</code> files (16-bit, 48kHz)</li>
  <li>8 labeled emotions:<br>
<code class="language-plaintext highlighter-rouge">neutral</code>, <code class="language-plaintext highlighter-rouge">calm</code>, <code class="language-plaintext highlighter-rouge">happy</code>, <code class="language-plaintext highlighter-rouge">sad</code>, <code class="language-plaintext highlighter-rouge">angry</code>, <code class="language-plaintext highlighter-rouge">fearful</code>, <code class="language-plaintext highlighter-rouge">disgust</code>, <code class="language-plaintext highlighter-rouge">surprised</code>
</li>
</ul>

<p>Each <code class="language-plaintext highlighter-rouge">.wav</code> file is preprocessed into a <strong>Mel spectrogram</strong> and stored as <code class="language-plaintext highlighter-rouge">.npy</code> format.</p>

<hr>

<h2 id="-model-architectures">🧠 Model Architectures</h2>

<h3 id="2-different-models">2 different models</h3>

<h4 id="-cnn-best-performance">✅ CNN (Best Performance)</h4>

<ul>
  <li>3x Conv1D + ReLU + MaxPool</li>
  <li>Fully connected layers</li>
  <li>Dropout regularization (adjustable)</li>
</ul>

<h4 id="-cnn--gru">🔁 CNN + GRU</h4>

<ul>
  <li>CNN front-end for spatial encoding</li>
  <li>GRU (recurrent layers) to capture temporal dynamics</li>
  <li>Lower accuracy than CNN-only model</li>
</ul>

<hr>

<h3 id="-specaugment-data-augmentation">🧪 SpecAugment: Data Augmentation</h3>

<p>To improve generalization, we implemented <code class="language-plaintext highlighter-rouge">SpecAugmentTransform</code> which applies:</p>

<ul>
  <li>🕒 <strong>Time masking</strong>: hides random time intervals</li>
  <li>📡 <strong>Frequency masking</strong>: hides random mel frequency bands</li>
</ul>

<hr>

<h3 id="-training-results">📈 Training Results</h3>

<ul>
  <li>Best Validation Accuracy: <strong>~49.6%</strong>
</li>
  <li>Training set: Actors 1–20</li>
  <li>Validation set: Actors 21–24</li>
</ul>

<p><strong>Confusion Matrix Example:</strong></p>

<p><img src="figures/confusion_matrix.png" alt="ConfusionMatrix" width="700"></p>

<h3 id="-key-observations">🔍 Key Observations:</h3>

<ul>
  <li>Surprised, calm, and disgust are the most accurately predicted emotions.</li>
  <li>Neutral, happy, and sad tend to be confused with each other, which is common due to subtle acoustic variations.</li>
  <li>The model struggles with fearful and angry in some cases — suggesting those may share overlapping vocal characteristics in this dataset.</li>
  <li>Emotion classes like happy and fearful are often misclassified due to variability in expression intensity among different actors.</li>
</ul>

<h5 id="-interpretation">📈 Interpretation</h5>

<p>While the model captures general emotion cues, it suffers from class overlap and limited generalization. The accuracy remains significantly above random (12.5% for 8 classes), but there is still room for improvement.</p>

<hr>

<h2 id="-getting-started">🚀 Getting Started</h2>

<h3 id="1-install-dependencies">1. Install dependencies</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<h3 id="2-download-dataset-from-kaggle">2. Download dataset from Kaggle</h3>

<p>Follow the instructions in the README.md located in the data folder</p>

<h3 id="3--train-the-model">3 . Train the model</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python src/train.py
</code></pre></div></div>

<h3 id="4--evaluation-the-performances-with-a-confusion-matrix">4.  Evaluation the performances with a confusion matrix</h3>

<p>```bash
python src/confusion_matrix.py</p>


  </article>
</div>
<div class="post page-break">
  <header class="post-header">
    <h1 class="post-title">FatiguEye
      
        <a href="https://github.com/Tirovo/FatiguEye" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fab fa-github" style="margin-left: 10px;margin-right: 10px;"></i></a>
      
    </h1>
    <p class="post-description">Detection of fatigue using a webcam</p>
  </header>

  <article>
    <p><img src="/assets/img/projects/FatiguEye/main.png" width="300" height="auto"></p>

<p>A smart computer vision system that detects signs of fatigue, eye strain, and microsleep using a standard webcam.</p>

<hr>

<h2 id="-purpose">🎯 Purpose</h2>

<p><strong>FatiguEye</strong> is a real-time fatigue detection system based on eye tracking and facial landmark analysis.<br>
It helps identify early signs of drowsiness by measuring:</p>
<ul>
  <li>👁️ Eye Aspect Ratio (EAR)</li>
  <li>🔁 Blink frequency</li>
  <li>⏱️ Prolonged eyelid closure</li>
  <li>⚠️ Microsleep events</li>
</ul>

<p>Ideal for driver monitoring, industrial safety, or ergonomic fatigue prevention.</p>

<hr>

<h2 id="-how-it-works">🧠 How It Works</h2>

<p>FatiguEye uses <a href="https://google.github.io/mediapipe/" rel="external nofollow noopener" target="_blank">MediaPipe Face Mesh</a> to extract eye landmarks, and computes the <strong>EAR (Eye Aspect Ratio)</strong> on each video frame.</p>

<h3 id="-processing-pipeline">📡 Processing pipeline:</h3>

<ol>
  <li>🎥 Webcam feed is captured in real-time</li>
  <li>🧠 Facial landmarks (eyes) are detected with Mediapipe</li>
  <li>📏 EAR is calculated per eye</li>
  <li>🧮 Blink count and eye closure duration are analyzed</li>
  <li>🔔 Fatigue alerts are raised (visual + audio)</li>
</ol>

<hr>

<h2 id="-demo-preview">🚀 Demo Preview</h2>

<blockquote>

</blockquote>

<p><img src="assets/demo.gif" alt="FatiguEye demo" width="700"></p>

<hr>

<h2 id="-technologies-used">💻 Technologies Used</h2>

<table>
  <thead>
    <tr>
      <th>Tech</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Python</td>
      <td>Core language</td>
    </tr>
    <tr>
      <td>OpenCV</td>
      <td>Webcam video processing + overlays</td>
    </tr>
    <tr>
      <td>MediaPipe</td>
      <td>Face mesh &amp; eye landmark detection</td>
    </tr>
    <tr>
      <td>NumPy</td>
      <td>EAR computation</td>
    </tr>
    <tr>
      <td>Streamlit</td>
      <td>Live web dashboard</td>
    </tr>
    <tr>
      <td>winsound</td>
      <td>Audio alert (Windows only)</td>
    </tr>
  </tbody>
</table>

<hr>

<h2 id="-installation">📦 Installation</h2>

<p>```bash
git clone https://github.com/Tirovo/fatigueye.git
cd fatigueye
python -m venv venv
source venv/bin/activate  # Or venv\Scriptsctivate on Windows</p>


  </article>
</div>
<div class="post page-break">
  <header class="post-header">
    <h1 class="post-title">VATN-WaterGame
      
        <a href="https://github.com/mpek29/VATN-WaterGame" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fab fa-github" style="margin-left: 10px;margin-right: 10px;"></i></a>
      
    </h1>
    <p class="post-description">💧 A strategic game on water management!</p>
  </header>

  <article>
    <p><strong>VATN</strong> (from the Old Norse word for “water”) is a game designed to raise awareness about water management. Players must make critical daily decisions to address major water-related issues in their country, influencing key parameters that determine the nation’s survival.</p>

<h2 id="-purpose">🎯 Purpose</h2>

<ul>
  <li>💧 <strong>Water Management Awareness</strong>: Educate players on the importance of sustainable water policies.</li>
  <li>🎮 <strong>Engaging Decision-Making</strong>: Every day presents a new challenge that affects the nation’s status.</li>
  <li>🏆 <strong>Survival &amp; Strategy</strong>: Keep your country alive as long as possible by maintaining stability.</li>
  <li>📊 <strong>Progress Tracking</strong>: Visualize the evolution of key parameters through in-game graphs.</li>
</ul>

<h2 id="-features">📝 Features</h2>

<table>
  <thead>
    <tr>
      <th>🏷️ Feature</th>
      <th>🔍 Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>🌎 <strong>Game Type</strong>
</td>
      <td>Strategic Decision-Making Simulation</td>
    </tr>
    <tr>
      <td>📅 <strong>Daily Choices</strong>
</td>
      <td>Players make decisions each day affecting country parameters</td>
    </tr>
    <tr>
      <td>📉 <strong>Dynamic Statistics</strong>
</td>
      <td>Key indicators fluctuate based on player actions</td>
    </tr>
    <tr>
      <td>💀 <strong>Game Over</strong>
</td>
      <td>The country collapses if the population reaches 0</td>
    </tr>
    <tr>
      <td>📂 <strong>Save &amp; Load</strong>
</td>
      <td>Resume previous games using saved files</td>
    </tr>
    <tr>
      <td>🏅 <strong>Rankings</strong>
</td>
      <td>Compare results with previous local games</td>
    </tr>
    <tr>
      <td>📊 <strong>Graphical Summary</strong>
</td>
      <td>Track country status evolution over time</td>
    </tr>
  </tbody>
</table>

<h2 id="-gameplay-overview">📺 Gameplay Overview</h2>

<table>
  <thead>
    <tr>
      <th>🎮 Main Menu</th>
      <th>📊 In-Game Statistics</th>
      <th>🏆 Endgame Rankings</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/img/projects/VATN-WaterGame/main_menu.png" width="300" height="auto"></td>
      <td><img src="/assets/img/projects/VATN-WaterGame/game_stats.png" width="300" height="auto"></td>
      <td><img src="/assets/img/projects/VATN-WaterGame/rankings.png" width="300" height="auto"></td>
    </tr>
  </tbody>
</table>


  </article>
</div>

  
      
      <!-- Récupération de l'ordre des sous-catégories définies --><div class="post page-break">
  <header class="post-header">
    <h1 class="post-title">AntiVuvuzelaFilter
      
        <a href="https://github.com/mpek29/AntiVuvuzelaFilter" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fab fa-github" style="margin-left: 10px;margin-right: 10px;"></i></a>
      
    </h1>
    <p class="post-description">🎵 Noise filter for clear audio 🎙️</p>
  </header>

  <article>
    <p><strong>Anti-Vuvuzela Filter</strong> is an open-source project dedicated to <strong>second-order analog filters</strong> and beyond. This project was initially developed to design an <strong>“anti-vuvuzela” filter</strong>, aiming to attenuate the distinctive and persistent sound of vuvuzelas while preserving the clarity of commentators’ voices during the <strong>2010 FIFA World Cup</strong>.</p>

<h2 id="-purpose">🎯 Purpose</h2>

<ul>
  <li>🔇 <strong>Targeted Noise Reduction</strong>: Specifically designed to <strong>attenuate vuvuzela noise</strong> while maintaining the intelligibility of speech.</li>
  <li>🎚 <strong>Second-Order Analog Filtering</strong>: Utilizing advanced filtering techniques for efficient noise cancellation.</li>
  <li>🛠️ <strong>Open-source and Customizable</strong>: Modify and adapt the design for other audio filtering applications.</li>
</ul>

<h2 id="-features">📝 Features</h2>

<table>
  <thead>
    <tr>
      <th>🏷️ Feature</th>
      <th>🔍 Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>🎼 <strong>Filter Type</strong>
</td>
      <td>Second-order analog filter</td>
    </tr>
    <tr>
      <td>🎯 <strong>Target Frequency</strong>
</td>
      <td>233 Hz (typical vuvuzela frequency)</td>
    </tr>
    <tr>
      <td>🎙 <strong>Voice Preservation</strong>
</td>
      <td>Maintains speech clarity</td>
    </tr>
    <tr>
      <td>🔧 <strong>Components</strong>
</td>
      <td>Resistors, capacitors, and operational amplifiers</td>
    </tr>
    <tr>
      <td>🖥️ <strong>Simulation Tools</strong>
</td>
      <td>Jupyter Notebook, LTSpice</td>
    </tr>
    <tr>
      <td>🛠 <strong>Real-world Testing</strong>
</td>
      <td>Assembled and tested in real conditions</td>
    </tr>
    <tr>
      <td>🔌 <strong>Input</strong>
</td>
      <td>Analog audio signal</td>
    </tr>
    <tr>
      <td>🔊 <strong>Output</strong>
</td>
      <td>Cleaned audio signal with reduced vuvuzela noise</td>
    </tr>
    <tr>
      <td>🌍 <strong>Use Cases</strong>
</td>
      <td>Audio signal processing, speech enhancement, noise reduction</td>
    </tr>
  </tbody>
</table>

<h2 id="-simulation--testing">📐 Simulation &amp; Testing</h2>

<table>
  <thead>
    <tr>
      <th>🛠️ LTSpice Circuit</th>
      <th>📜 Simulation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="/assets/img/projects/AntiVuvuzelaFilter/assembly.png" width="300" height="auto"></td>
      <td><img src="/assets/img/projects/AntiVuvuzelaFilter/simulation.png" width="300" height="auto"></td>
    </tr>
  </tbody>
</table>


  </article>
</div>
<div class="post page-break">
  <header class="post-header">
    <h1 class="post-title">MotorControlShield
      
        <a href="https://github.com/Tirovo/MotorControlShield" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fab fa-github" style="margin-left: 10px;margin-right: 10px;"></i></a>
      
    </h1>
    <p class="post-description">🔄 Arduino shield for single DC motor control</p>
  </header>

  <article>
    <p>The <strong>Motor Control Shield</strong> is an open-source project designed for controlling DC motors. It comes in the form of an Arduino shield mounted on an STM32 Nucleo board. The shield enables motor control via an NMOS transistor, current measurement with a shunt resistor, and rotation tracking using data from an incremental encoder.</p>

<h2 id="-purpose">🎯 Purpose</h2>

<ul>
  <li>🔄 <strong>Motor Control</strong>: Provides precise control over DC motors.</li>
  <li>📉 <strong>Current Measurement</strong>: Monitors the current consumed by the motor.</li>
  <li>🔄 <strong>Rotation Tracking</strong>: Uses an incremental encoder to track motor rotation.</li>
  <li>🛠️ <strong>Open-source &amp; Customizable</strong>: Modifiable and adaptable for various projects.</li>
</ul>

<h2 id="-features">📝 Features</h2>

<table>
  <thead>
    <tr>
      <th>🏷️ Feature</th>
      <th>🔍 Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>🔄 <strong>Motor Control</strong>
</td>
      <td>Uses an NMOS transistor to control motor speed and direction</td>
    </tr>
    <tr>
      <td>📉 <strong>Current Measurement</strong>
</td>
      <td>Shunt resistor for measuring the current consumed by the motor</td>
    </tr>
    <tr>
      <td>🔄 <strong>Rotation Tracking</strong>
</td>
      <td>Incremental encoder to track motor position and rotation speed</td>
    </tr>
    <tr>
      <td>🔘 <strong>Compatibility</strong>
</td>
      <td>Arduino shield compatible with STM32 Nucleo boards</td>
    </tr>
    <tr>
      <td>🖥️ <strong>PCB Design</strong>
</td>
      <td>Open-source and customizable</td>
    </tr>
    <tr>
      <td>🌍 <strong>Use Cases</strong>
</td>
      <td>robots, embedded systems, and motor control applications</td>
    </tr>
  </tbody>
</table>

<h2 id="-pcb-design-preview">📐 PCB Design Preview</h2>

<table>
  <thead>
    <tr>
      <th>📜 Functional diagram</th>
      <th>📜 Schematic</th>
      <th>🖥️ PCB Layout</th>
      <th>🏗️ 3D</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="assets/img/functional_diagram.png" alt="Schematic"></td>
      <td><img src="assets/img/schematic.png" alt="Schematic"></td>
      <td><img src="assets/img/pcb_layout.png" alt="PCB Layout"></td>
      <td><img src="assets/img/3d.png" alt="3D"></td>
    </tr>
  </tbody>
</table>


  </article>
</div>

  
  </div>

<hr>


          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 LONDE  Tristan. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
