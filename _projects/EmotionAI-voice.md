---
layout: page
title: EmotionAI-voice
description: An AI-powered application for detecting human emotions
full_name: Tirovo/EmotionAI-voice
img: assets/img/projects/EmotionAI-voice/main.png
importance: 1
git: https://github.com/Tirovo/EmotionAI-voice
github: https://github.com/Tirovo/EmotionAI-voice
category: Computer Science
subcategory: Artificial Intelligence
---


**EmotionAI Voice** is an open-source deep learning project that classifies vocal emotions using raw `.wav` audio.  
It's designed for applications in mental health monitoring, UX analysis, and intelligent speech interfaces.

ğŸ”¬ The model is trained **from scratch**, using spectrogram-based audio features, and aims to recognize 8 core emotions.

---

## ğŸ¯ Features


- ğŸ§  Emotion recognition: `neutral`, `calm`, `happy`, `sad`, `angry`, `fearful`, `disgust`, `surprised`
- ğŸ§ Accepts `.wav` audio inputs (from RAVDESS dataset)
- ğŸ“Š CNN and CNN+GRU models implemented in PyTorch
- ğŸ” Real-time evaluation with confusion matrix and accuracy tracking
- ğŸ› ï¸ Fully open-source and customizable (no pre-trained models)
- ğŸ§ª Includes **SpecAugment** for data augmentation (frequency/time masking)

---

## ğŸ“š Dataset â€” RAVDESS


We use the [**RAVDESS** dataset](https://zenodo.org/record/1188976), which includes:

- ğŸ­ 24 professional actors (balanced male/female)
- ğŸ™ï¸ 1440 `.wav` files (16-bit, 48kHz)
- 8 labeled emotions:  
  `neutral`, `calm`, `happy`, `sad`, `angry`, `fearful`, `disgust`, `surprised`

Each `.wav` file is preprocessed into a **Mel spectrogram** and stored as `.npy` format.

---

## ğŸ§  Model Architectures


### 2 different models


#### âœ… CNN (Best Performance)


- 3x Conv1D + ReLU + MaxPool
- Fully connected layers
- Dropout regularization (adjustable)

#### ğŸ” CNN + GRU


- CNN front-end for spatial encoding
- GRU (recurrent layers) to capture temporal dynamics
- Lower accuracy than CNN-only model

---

### ğŸ§ª SpecAugment: Data Augmentation



To improve generalization, we implemented `SpecAugmentTransform` which applies:

- ğŸ•’ **Time masking**: hides random time intervals
- ğŸ“¡ **Frequency masking**: hides random mel frequency bands

---

### ğŸ“ˆ Training Results



- Best Validation Accuracy: **~49.6%**
- Training set: Actors 1â€“20
- Validation set: Actors 21â€“24


**Confusion Matrix Example:**

<img src="figures/confusion_matrix.png" alt="ConfusionMatrix" width="700">

### ğŸ” Key Observations:


- Surprised, calm, and disgust are the most accurately predicted emotions.
- Neutral, happy, and sad tend to be confused with each other, which is common due to subtle acoustic variations.
- The model struggles with fearful and angry in some cases â€” suggesting those may share overlapping vocal characteristics in this dataset.
- Emotion classes like happy and fearful are often misclassified due to variability in expression intensity among different actors.

##### ğŸ“ˆ Interpretation


While the model captures general emotion cues, it suffers from class overlap and limited generalization. The accuracy remains significantly above random (12.5% for 8 classes), but there is still room for improvement.

---

## ğŸš€ Getting Started


### 1. Install dependencies



```bash
pip install -r requirements.txt
```

### 2. Download dataset from Kaggle



Follow the instructions in the README.md located in the data folder

### 3 . Train the model



```bash
python src/train.py
```

### 4.  Evaluation the performances with a confusion matrix



```bash
python src/confusion_matrix.py

